{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.io as pio\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.ops import xyxy2xywhn\n",
    "\n",
    "pio.renderers.default = 'png'\n",
    "pio.templates.default = 'plotly_dark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>utf8_string</th>\n",
       "      <th>points</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a4ea732cd3d5948a_1</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>[525.83, 3.4, 197.64, 33.94]</td>\n",
       "      <td>Performance</td>\n",
       "      <td>[525.83, 3.4, 723.47, 7.29, 722.76, 36.99, 525...</td>\n",
       "      <td>6707.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a4ea732cd3d5948a_2</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>[534.67, 64.68, 91.22, 38.19]</td>\n",
       "      <td>Sport</td>\n",
       "      <td>[535.73, 64.68, 623.41, 67.51, 625.89, 102.87,...</td>\n",
       "      <td>3483.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a4ea732cd3d5948a_3</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>[626.95, 63.62, 96.52, 31.82]</td>\n",
       "      <td>Watch</td>\n",
       "      <td>[626.95, 63.62, 721.7, 63.62, 723.47, 95.44, 6...</td>\n",
       "      <td>3071.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4ea732cd3d5948a_4</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>[577.4, 141.87, 147.13, 43.1]</td>\n",
       "      <td>...period.</td>\n",
       "      <td>[580.02, 143.61, 724.53, 141.87, 723.66, 184.9...</td>\n",
       "      <td>6341.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a4ea732cd3d5948a_5</td>\n",
       "      <td>a4ea732cd3d5948a</td>\n",
       "      <td>[391.03, 163.9, 60.82, 38.65]</td>\n",
       "      <td>.</td>\n",
       "      <td>[395.2, 163.9, 451.85, 191.94, 445.59, 202.55,...</td>\n",
       "      <td>2350.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id          image_id                           bbox  \\\n",
       "0  a4ea732cd3d5948a_1  a4ea732cd3d5948a   [525.83, 3.4, 197.64, 33.94]   \n",
       "1  a4ea732cd3d5948a_2  a4ea732cd3d5948a  [534.67, 64.68, 91.22, 38.19]   \n",
       "2  a4ea732cd3d5948a_3  a4ea732cd3d5948a  [626.95, 63.62, 96.52, 31.82]   \n",
       "3  a4ea732cd3d5948a_4  a4ea732cd3d5948a  [577.4, 141.87, 147.13, 43.1]   \n",
       "4  a4ea732cd3d5948a_5  a4ea732cd3d5948a  [391.03, 163.9, 60.82, 38.65]   \n",
       "\n",
       "   utf8_string                                             points     area  \n",
       "0  Performance  [525.83, 3.4, 723.47, 7.29, 722.76, 36.99, 525...  6707.90  \n",
       "1        Sport  [535.73, 64.68, 623.41, 67.51, 625.89, 102.87,...  3483.69  \n",
       "2        Watch  [626.95, 63.62, 721.7, 63.62, 723.47, 95.44, 6...  3071.27  \n",
       "3   ...period.  [580.02, 143.61, 724.53, 141.87, 723.66, 184.9...  6341.30  \n",
       "4            .  [395.2, 163.9, 451.85, 191.94, 445.59, 202.55,...  2350.69  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = '/mnt/s/CV/STR'\n",
    "images_path = os.path.join(dataset_path, 'images')\n",
    "\n",
    "df = pd.read_csv(os.path.join(dataset_path, 'annot.csv'), index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_images = pd.Series(df['image_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, val_imgs = train_test_split(unique_images, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['image_id'].isin(train_imgs)].copy()\n",
    "val_df = df[df['image_id'].isin(val_imgs)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov8_dataset_path = '/mnt/s/CV/STR_yolo'\n",
    "yolov8_images_path = os.path.join(yolov8_dataset_path, 'images')\n",
    "yolov8_labels_path = os.path.join(yolov8_dataset_path, 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yolo_dataset(df, yolov8_images_path, yolov8_labels_path, sample):\n",
    "    for unique_image in tqdm(df['image_id'].unique()):\n",
    "        if not os.path.exists(os.path.join(yolov8_images_path, sample, unique_image + '.jpg')):\n",
    "            shutil.copy(os.path.join(images_path, unique_image + '.jpg'), os.path.join(yolov8_images_path, sample, unique_image + '.jpg'))\n",
    "        label_file = os.path.join(yolov8_labels_path, sample, unique_image + '.txt')\n",
    "        height, width = cv.imread(os.path.join(images_path, unique_image + '.jpg')).shape[:2]\n",
    "        with open(label_file, 'w') as f:\n",
    "            for _, row in df[df['image_id'] == unique_image].iterrows():\n",
    "                bbox = np.array(row['bbox'].strip('[]').split(', '), dtype=float)\n",
    "                xyxy = np.array([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
    "                xywhn = xyxy2xywhn(xyxy, w=width, h=height, clip=True)\n",
    "                f.writelines(f'0 {xywhn[0]} {xywhn[1]} {xywhn[2]} {xywhn[3]}\\n')\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_yolo_dataset(train_df, yolov8_images_path, yolov8_labels_path, 'train')\n",
    "# create_yolo_dataset(val_df, yolov8_images_path, yolov8_labels_path, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8l.pt\", task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.35 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.14 ðŸš€ Python-3.11.5 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12288MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=yolo.yaml, epochs=3, time=None, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 22:13:03.689506: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-19 22:13:03.689546: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-19 22:13:03.690015: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5583571  ultralytics.nn.modules.head.Detect           [1, [256, 512, 512]]          \n",
      "Model summary: 365 layers, 43630611 parameters, 43630595 gradients, 165.4 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/screamer/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning:\n",
      "\n",
      "Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/s/CV/STR_yolo/labels/train.cache... 17422 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17422/17422 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /mnt/s/CV/STR_yolo/images/train/fc7029d496074b05.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/s/CV/STR_yolo/labels/val.cache... 4356 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4356/4356 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /mnt/s/CV/STR_yolo/images/val/21b6bde202d35e2d.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4356 [00:00<?, ?it/s]/home/screamer/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning:\n",
      "\n",
      "Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "\n",
      "        1/3      10.7G      1.695      1.188      1.142         64        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4356/4356 [07:21<00:00,  9.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 545/545 [00:50<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4356     201746      0.682      0.473      0.519      0.288\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/3       9.3G      1.575      1.062      1.093        106        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4356/4356 [06:55<00:00, 10.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 545/545 [00:40<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4356     201746      0.711      0.505      0.562      0.325\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3      8.32G      1.462     0.9559      1.053         58        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4356/4356 [06:47<00:00, 10.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 545/545 [02:10<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4356     201746      0.737      0.544      0.607      0.357\n",
      "\n",
      "3 epochs completed in 0.415 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 87.6MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 87.6MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.14 ðŸš€ Python-3.11.5 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12288MiB)\n",
      "Model summary (fused): 268 layers, 43607379 parameters, 0 gradients, 164.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/screamer/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning:\n",
      "\n",
      "Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 545/545 [01:08<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4356     201746      0.736      0.544      0.607      0.356\n",
      "Speed: 0.1ms preprocess, 4.9ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = model.train(data='yolo.yaml', epochs=3, imgsz=640, batch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /mnt/s/CV/STR/images/fdefa17d92c7a54a.jpg: 640x640 56 texts, 198.6ms\n",
      "Speed: 2.8ms preprocess, 198.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/train5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(images_path + '/fdefa17d92c7a54a.jpg', save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
